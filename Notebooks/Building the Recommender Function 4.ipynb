{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8f8cc2",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "Now that I have my pickled models I can load them in and create some recommendation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f98c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import scipy as sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9316b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from scipy.sparse import csr_matrix\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Function to reduce the memory usage of a DataFrame.\n",
    "def reduce_memory(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        if df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "# Generator function to load data in chunks.\n",
    "def data_generator(df, chunksize=10000):\n",
    "    for i in range(0, df.shape[0], chunksize):\n",
    "        yield df.iloc[i:i+chunksize]\n",
    "\n",
    "\n",
    "games = reduce_memory(pd.read_csv('../Data/games.csv'))\n",
    "recommendations = reduce_memory(pd.read_csv('../Data/recommendations.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ddc471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading composite rating algorithm\n",
    "\n",
    "with open('../Models/composite_rating_predictor.pkl', 'rb') as file:\n",
    "    comp_algo = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def90c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading is recommended algorithm\n",
    "\n",
    "with open('../Models/recommender_predictor.pkl', 'rb') as file:\n",
    "    recommender_algo = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6ae987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated rating for user 987098 and item 359550 is 4.745550590913284\n"
     ]
    }
   ],
   "source": [
    "from surprise import PredictionImpossible # just in case I hit a user or item that doesn't exist it'll flag impossible \n",
    "# instead of returning a random guess \n",
    "\n",
    "\n",
    "# this is my composite rating algorithm, the current user and item value included are a random user who put in 900 hours \n",
    "# into Rainbow Six Siege (the game listed) yet despite that rated it negatively.  This composite rating of mine takes that \n",
    "# into consideration and gave him a much lower predicted rating.\n",
    "user_id = 987098\n",
    "item_id = 359550\n",
    "\n",
    "\n",
    "# SVD appears to return default scores for items and users that do not exist.  This if statement is meant to avoid that.\n",
    "if user_id not in recommendations['user_id'].unique():\n",
    "    print(f\"Error: User {user_id} does not exist in the training set.\")\n",
    "elif item_id not in recommendations['app_id'].unique():\n",
    "    print(f\"Error: Item {item_id} does not exist in the training set.\")\n",
    "else:\n",
    "    loaded_pred = comp_algo.predict(user_id, item_id)\n",
    "    print(f\"The estimated rating for user {user_id} and item {item_id} is {loaded_pred.est}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c8a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated rating for user 987098 and item 359550 is 0.8679839396964601\n"
     ]
    }
   ],
   "source": [
    "# this is the is_recommended algorithm.  Since it also learns implicit features it recognizes that this user has put a ton\n",
    "# of time into this game and rates it higher despite the user rating it negatively. It brings up the issue of subjectivity,\n",
    "# did this user not recommend the game to be funny?  Is there something the creators did which altered his opinion?\n",
    "# Neither results are necessarily incorrect, they both take the information they're presented with just weighted in different\n",
    "# manners.  \n",
    "\n",
    "user_id = 987098\n",
    "item_id = 359550\n",
    "\n",
    "if user_id not in recommendations['user_id'].unique():\n",
    "    print(f\"Error: User {user_id} does not exist in the training set.\")\n",
    "elif item_id not in recommendations['app_id'].unique():\n",
    "    print(f\"Error: Item {item_id} does not exist in the training set.\")\n",
    "else:\n",
    "    loaded_pred = recommender_algo.predict(user_id, item_id)\n",
    "    print(f\"The estimated rating for user {user_id} and item {item_id} is {loaded_pred.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60845ac6",
   "metadata": {},
   "source": [
    "I wrote above in comments but I'll just summarize here.  Both algorithms learned in different ways depending on the implicit features they absorbed through SVDpp.  The algorithm that relies on the composite rating I generated notices the flags the user put up and thusly penalizes them, agreeing that because the user meets a certain criteria and they don't recommend the game in question that it should be rated lower.  The is recommended algorithm implicitly realizes that even if that user doesn't recommend the game they've put a ton of time into it and thus it gets a rating closer to 1 than 0.  Neither is necessarily incorrect, they simply learned different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19bddea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of two functions to get a top 10 list of estimated ratings for a user based on what it predicts the user would like\n",
    "# this one is for the comp_algo with a scale of 0-10.\n",
    "\n",
    "\n",
    "\n",
    "# Returns the list of games owned by whatever user_id is keyed into the final function. Shared among both algorithms.\n",
    "def get_owned_games(user_id, recommendations):\n",
    "    return recommendations[recommendations['user_id'] == user_id]['app_id'].unique()\n",
    "\n",
    "# unique per algorithm, gets the predictions of the user that is keyed in to the final function\n",
    "def predict_comp_ratings(user_id, games_to_predict):\n",
    "    predictions = [(game, comp_algo.predict(user_id, game).est) for game in games_to_predict]\n",
    "    return sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# function that combines all the output \n",
    "def recommend_comp_games(user_id):\n",
    "    \n",
    "    # Check if user_id exists in the training set\n",
    "    # Unfortunately without this the algorithm will just return a default list for anyone.\n",
    "    if user_id not in recommendations['user_id'].unique():\n",
    "        print(f\"Error: user {user_id} does not exist in the training set.\")\n",
    "        return []\n",
    "\n",
    "    # My model appears to have learned a little too well, it has a tendency to recommend the same games to everyone \n",
    "    # because it has found that those games perform almost unanimously well. This popularity metric changes up the output \n",
    "    # by a fair amount.\n",
    "    popular_games = games[games['user_reviews'] > 10000]\n",
    "\n",
    "    # Collects the list of games for the user from the first function.\n",
    "    owned_games = get_owned_games(user_id, recommendations)\n",
    "\n",
    "    # Filter out the games already owned by the user and cross-reference it with the total number of reviews the game has\n",
    "    games_to_predict = popular_games[~popular_games['app_id'].isin(owned_games)]['app_id']\n",
    "\n",
    "    # Predict ratings for the filtered games\n",
    "    top_predictions = predict_comp_ratings(user_id, games_to_predict)[:10]\n",
    "\n",
    "    # Takes the id's of the games returned and maps them to their title for easier legibility, additionally includes \n",
    "    # the models predicted rating for the game by the user \n",
    "    top_games_with_ratings = [(games[games['app_id'] == game_id]['title'].iloc[0], rating) \n",
    "                              for game_id, rating in top_predictions]\n",
    "\n",
    "    return top_games_with_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "139d1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identical function but for the rec_algo which is a 0-1 scale.\n",
    "\n",
    "def predict_recommended_ratings(user_id, games_to_predict):\n",
    "    \n",
    "    # since its a 0-1 scale have to ensure rating returns as a float to ensure a better idea of whether a game is recommended\n",
    "    # or not.\n",
    "    predictions = [(game, float(recommender_algo.predict(user_id, game).est)) for game in games_to_predict]\n",
    "    return sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def recommend_rec_games(user_id):\n",
    "     \n",
    "        \n",
    "    if user_id not in recommendations['user_id'].unique():\n",
    "        print(f\"Error: user {user_id} does not exist in the training set.\")\n",
    "        return []\n",
    "    \n",
    "    owned_games = get_owned_games(user_id, recommendations)\n",
    "    \n",
    "    # once again the popularity metric penalizes the model for recommending low played but high ranking games to the user\n",
    "    games_to_predict = games[(~games['app_id'].isin(owned_games)) & (games['user_reviews'] > 10000)]['app_id']\n",
    "    top_predictions = predict_recommended_ratings(user_id, games_to_predict)[:10]\n",
    "    \n",
    "    top_games_with_ratings = [(games[games['app_id'] == game_id]['title'].iloc[0], rating) \n",
    "                              for game_id, rating in top_predictions]\n",
    "    return top_games_with_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5a2b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A Short Hike', 0.9924125343543445),\n",
       " ('Aseprite', 0.9883104327435392),\n",
       " ('Rhythm Doctor', 0.9880431602954187),\n",
       " ('Terraria', 0.9864730291409904),\n",
       " ('Hades', 0.9856765131629152),\n",
       " ('Vampire Survivors', 0.9847364608411153),\n",
       " ('Entropy : Zero 2', 0.9846111366114456),\n",
       " ('OneShot', 0.9839081028437245),\n",
       " ('Portal 2', 0.9837804406334922),\n",
       " ('Finding Paradise', 0.9836854660264128)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_rec_games(0) # calling the 0-1 scale algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3970c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kung Fury', 7.893856197449941),\n",
       " ('Finding Paradise', 7.875466930301883),\n",
       " ('OneShot', 7.8386029072420085),\n",
       " ('Call of Juarez: Gunslinger', 7.824269883694496),\n",
       " ('Gorogoa', 7.818253135815099),\n",
       " ('Montaro', 7.779746047001589),\n",
       " ('Eternal Senia', 7.754772580247042),\n",
       " ('A Short Hike', 7.739986974339837),\n",
       " ('Quake', 7.738840422621572),\n",
       " ('Fran Bow', 7.7072577325659966)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_comp_games(0) # calling the 0-10 scale algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d0f73",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The models are rather tricky, with no threshholds or limits holding them back they unanimously select the top performing games (in its opinion) and recommend them frequently to almost every player regardless of their preferences.  Yet despite that, their estimated guessings of scores for players are fairly consistent and give me results I would expect.  The 0-1 scale recommender seems a bit better at finding unanimously popular games whereas the composite rating algorithm takes some user nuances into account and returns some interesting and unexpected results.\n",
    "\n",
    "I don't think any of them on their own would perform very well in a real setting and that's because I think the model could be very benefitted by combined models.  Models I can see assisting and refining the output would be a similar users matrix utilizing KNN, tag based filtering (potentially an item similarity matrix using tags and vectorization), and some more intuitive model thresholds.  Popularity was a good start to threshholding but adding more features would improve the model significantly.\n",
    "\n",
    "Overall the models do the task that they were trained to do.  They just do not understand the complexities between each user and item.  That information can be provided through other means at a later date, for now the ability to understand how a user would rate a certain item is sufficient.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
